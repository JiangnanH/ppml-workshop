{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement:\n",
    "- numpy 1.19.5\n",
    "- tensorflow 2.5.0\n",
    "- tensorflow_privacy 0.6.1\n",
    "- sklearn 0.24.2\n",
    "\n",
    "Before performing the experiments, you need to fit the **pickle_file**(path for tabular QMNIST data) according to your own PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iYdgdZMocHNk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random as python_random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuMDw0xmAH-d"
   },
   "source": [
    "# Load QMNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOBOLaM6ghKD",
    "outputId": "4191ad8b-6f65-4ad6-87e6-fc1d695cb81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/home/jiangnan/Desktop/ppml-workshop/data/QMNIST_tabular_ppml.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  x_defender = pickle_data['x_defender']\n",
    "  x_reserve = pickle_data['x_reserve']\n",
    "  y_defender = pickle_data['y_defender']\n",
    "  y_reserve = pickle_data['y_reserve']\n",
    "  del pickle_data\n",
    "print('Data loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qW1n24bvs6nP"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "y_defender = y_defender[:,0]\n",
    "y_reserve = y_reserve[:,0]\n",
    "\n",
    "#y_defender = np.expand_dims(y_defender,axis=1)\n",
    "#y_reserve = np.expand_dims(y_reserve,axis=1)\n",
    "\n",
    "#y_defender = tf.keras.utils.to_categorical(y_defender, num_classes=NUM_CLASSES)\n",
    "#y_reserve = tf.keras.utils.to_categorical(y_reserve, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TfaWzhkATJq"
   },
   "source": [
    "# Defender model $M_D$\n",
    "\n",
    "import the defender model which need to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "WjQgTR5vcerZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def defender_model_fn():\n",
    "    \"\"\"The architecture of the defender (victim) model.\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "    \n",
    "    #random_seeds = random.sample(range(0,100), 2)\n",
    "    #np.random.seed(random_seeds[0])\n",
    "    #python_random.seed(random_seeds[1])\n",
    "    \n",
    "    #model = LogisticRegression(random_state=None)\n",
    "    #model = RidgeClassifier(random_state=None)\n",
    "    #model = LinearSVC(random_state=None)\n",
    "    #model = DecisionTreeClassifier(random_state=None)\n",
    "    #model = KNeighborsClassifier() # no random_state\n",
    "    #model = KDTree()\n",
    "    #model = GaussianNB() # no random_state\n",
    "    model = SVC(probability=True,random_state=None)\n",
    "    #model = SGDClassifier(random_state=None)\n",
    "    #model = Perceptron(random_state=None)\n",
    "    #model = RandomForestClassifier(random_state=None)\n",
    "    #model = MLPClassifier(random_state=None)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# if random_select, randomly extract two points from both defender & reserve dataset, and repeat for n_extract times\n",
    "random_select = True\n",
    "n_extract = 100\n",
    "\n",
    "#if given_index, the attack will put the two left out points back to their original place\n",
    "given_index = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZg1y29Y_-bp"
   },
   "source": [
    "# Oracle attack model $M_A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "utePp7mWuXNy"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_defender_models(defender, data_in, data_out, n_records = 48, random_select = True, n_extract = 100, given_index = False):\n",
    "    \n",
    "    difference_in = []\n",
    "    difference_out = []\n",
    "    \n",
    "    number_loop = 0\n",
    "    \n",
    "    if random_select == True:\n",
    "        number_loop = n_extract\n",
    "    else:\n",
    "        number_loop = data_in[0].shape[0]\n",
    "        \n",
    "    for i in tqdm(range(number_loop)):\n",
    "\n",
    "        if random_select == True:\n",
    "            index = random.randint(0,n_records-1)\n",
    "        else:\n",
    "            index = i\n",
    "\n",
    "        evaluation_data_in = data_in[0][index]\n",
    "        evaluation_label_in = data_in[1][index]\n",
    "\n",
    "        evaluation_data_out = data_out[0][index]\n",
    "        evaluation_label_out = data_out[1][index]\n",
    "\n",
    "        evaluation_data = np.array([evaluation_data_in, evaluation_data_out])\n",
    "        evaluation_label = np.array([evaluation_label_in, evaluation_label_out])\n",
    "\n",
    "        evaluation = evaluation_data, evaluation_label\n",
    "\n",
    "\n",
    "        attack_train_data_in = np.delete(data_in[0], index, axis=0)\n",
    "        attack_train_label_in = np.delete(data_in[1], index, axis=0)\n",
    "\n",
    "        attack_in = attack_train_data_in, attack_train_label_in\n",
    "\n",
    "\n",
    "        attack_train_data_out = np.delete(data_out[0], index, axis=0)\n",
    "        attack_train_label_out = np.delete(data_out[1], index, axis=0)\n",
    "\n",
    "        attack_out = attack_train_data_out, attack_train_label_out\n",
    "\n",
    "        #predict = defender_model.predict(attack_in[0])\n",
    "        predict = defender_model.predict_proba(attack_in[0])\n",
    "        \n",
    "        if given_index == True:\n",
    "\n",
    "            attack_in_plus_one_in = np.insert(attack_in[0], index, evaluation[0][0].reshape(1,attack_in[0].shape[1]), axis=0), np.insert(attack_in[1], index, evaluation[1][0], axis=0)\n",
    "            attack_in_plus_one_out = np.insert(attack_in[0], index, evaluation[0][1].reshape(1,attack_in[0].shape[1]), axis=0), np.insert(attack_in[1], index, evaluation[1][1], axis=0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            attack_in_plus_one_in = np.vstack((evaluation[0][0].reshape(1,attack_in[0].shape[1]),attack_in[0])), np.hstack(( evaluation[1][0],attack_in[1]))\n",
    "            attack_in_plus_one_out = np.vstack((evaluation[0][1].reshape(1,attack_in[0].shape[1]),attack_in[0])), np.hstack(( evaluation[1][1],attack_in[1]))\n",
    "\n",
    "        \n",
    "        M_cD_in = defender_model_fn()\n",
    "        M_cD_out = defender_model_fn()\n",
    "\n",
    "        M_cD_in.fit(attack_in_plus_one_in[0], attack_in_plus_one_in[1])\n",
    "        M_cD_out.fit(attack_in_plus_one_out[0], attack_in_plus_one_out[1])\n",
    "        \n",
    "        #M_cD_in_predict = M_cD_in.predict(attack_in[0])\n",
    "        #M_cD_out_predict = M_cD_out.predict(attack_in[0])\n",
    "        \n",
    "        M_cD_in_predict = M_cD_in.predict_proba(attack_in[0])\n",
    "        M_cD_out_predict = M_cD_out.predict_proba(attack_in[0])\n",
    "\n",
    "        diff_in = np.mean(np.linalg.norm(M_cD_in_predict-predict))\n",
    "        diff_out = np.mean(np.linalg.norm(M_cD_out_predict-predict))\n",
    "\n",
    "        difference_in.append(diff_in)\n",
    "        difference_out.append(diff_out)\n",
    "    \n",
    "    return difference_in, difference_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:20<00:00,  4.40s/it]\n",
      "100%|██████████| 100/100 [07:15<00:00,  4.36s/it]\n",
      "100%|██████████| 100/100 [07:09<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy_in_all = []\n",
    "accuracy_out_all = []\n",
    "\n",
    "difference_in_all = []\n",
    "difference_out_all = []\n",
    "\n",
    "n_records = 1600\n",
    "n_trials = 3\n",
    "\n",
    "for i in range(n_trials):\n",
    "    \n",
    "    random_indexes = random.sample(range(0,200000), n_records)\n",
    "    data_in = x_defender[random_indexes], y_defender[random_indexes]\n",
    "    data_out = x_reserve[random_indexes], y_reserve[random_indexes]\n",
    "    \n",
    "    \n",
    "    defender_model = defender_model_fn()\n",
    "    defender_model.fit(data_in[0],data_in[1])\n",
    "    \n",
    "    predict_in_proba = defender_model.predict(data_in[0])\n",
    "    #predict_in = np.argmax(predict_in_proba, axis=1)\n",
    "    acc_in = accuracy_score(data_in[1], predict_in_proba)\n",
    "    \n",
    "    \n",
    "    predict_out_proba = defender_model.predict(data_out[0])\n",
    "    #predict_out = np.argmax(predict_out_proba, axis=1)\n",
    "    acc_out = accuracy_score(data_out[1], predict_out_proba)\n",
    "    \n",
    "    '''\n",
    "    auc_by_class = []\n",
    "    # compute auc per class then take the average value\n",
    "    for i in range(NUM_CLASSES):\n",
    "      class_indices = data_out[1] == i\n",
    "      if np.sum(class_indices) == 0:\n",
    "        continue\n",
    "      fpr, tpr, thresholds = metrics.roc_curve(class_indices, predict_reserve_proba[:,i])\n",
    "      auc = metrics.auc(fpr, tpr)\n",
    "      auc_by_class.append(auc)\n",
    "    \n",
    "    average_auc = np.mean(auc_by_class)\n",
    "    utility = max(2*average_auc -1,0)\n",
    "    \n",
    "    defender_acc_all.append(acc)\n",
    "    defender_auc_all.append(average_auc)\n",
    "    '''\n",
    "    \n",
    "    accuracy_in_all.append(acc_in)\n",
    "    accuracy_out_all.append(acc_out)\n",
    "    \n",
    "    \n",
    "    difference_in, difference_out = create_mock_defender_models(defender = defender_model,\n",
    "                                                                    data_in = data_in,\n",
    "                                                                    data_out = data_out,\n",
    "                                                                    n_records = n_records,\n",
    "                                                                    random_select = random_select,\n",
    "                                                                    n_extract = n_extract,\n",
    "                                                                    given_index = given_index)\n",
    "    \n",
    "    difference_in_all.append(difference_in)\n",
    "    difference_out_all.append(difference_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.914375, 0.909375, 0.91875]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_out_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9385416666666667\n",
      "0.9187500000000001\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(accuracy_in_all))\n",
    "print(np.mean(accuracy_out_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 18380.75it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 19749.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 22223.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute the privacy values by comparing all model pairs\n",
    "\n",
    "privacy_all = []\n",
    "variance_all = []\n",
    "sigma_error_all = []\n",
    "\n",
    "for i in range(len(difference_in_all)):\n",
    "    \n",
    "    difference_in = difference_in_all[i]\n",
    "    difference_out = difference_out_all[i]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for j in tqdm(range(len(difference_in))):\n",
    "        for k in range(len(difference_out)):\n",
    "\n",
    "            if difference_in[j] <= difference_out[k]:\n",
    "                results.append(1)\n",
    "            else:\n",
    "                results.append(0)\n",
    "\n",
    "    n = len(results)\n",
    "    p = 1-np.sum(results)/n\n",
    "\n",
    "    privacy = min(2*p,1)\n",
    "    variance = 2*p*(1-p)/n\n",
    "    sigma_error = np.sqrt(p*(1-p)/2*n)\n",
    "    \n",
    "    privacy_all.append(privacy)\n",
    "    variance_all.append(variance)\n",
    "    sigma_error_all.append(sigma_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7530666666666667\n",
      "4.6494383999999996e-05\n",
      "34.0851958338298\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(privacy_all))\n",
    "print(np.mean(variance_all))\n",
    "print(np.mean(sigma_error_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09499999999999997, 0.0, 0.21419999999999995]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Easy_NewMIA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
